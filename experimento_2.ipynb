{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFG FER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contiene el código necesario para la creación de un modelo de clasificación de imágenes de rostros humanos en función del estres emocional que presentan. Para ello, se va a utuilzar un dataset de imágenes pero de tipo .npy que contiene las imágenes de los rostros pero con un formato diferente al habitual, ya que las imágenes están en formato de matriz de píxeles. \n",
    "\n",
    "Los pasos a seguir para la creación del modelo son los siguientes:\n",
    "\n",
    "1. Importar las librerías necesarias\n",
    "2. Cargar el dataset\n",
    "3. Preprocesamiento de los datos\n",
    "4. Creación del modelo\n",
    "5. Entrenamiento del modelo\n",
    "6. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:34:48.177643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 13:34:48.933149: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/fgalan/miniconda3/lib/:/home/fgalan/miniconda3/lib/:/home/fgalan/miniconda3/envs/myenv/lib/\n",
      "2024-07-17 13:34:48.933258: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/fgalan/miniconda3/lib/:/home/fgalan/miniconda3/lib/:/home/fgalan/miniconda3/envs/myenv/lib/\n",
      "2024-07-17 13:34:48.933264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:34:50.007497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:34:50.007814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:34:50.016756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:34:50.016998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:34:50.017217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:34:50.017429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configurar TensorFlow para que use la CPU\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Ahora, al crear sesiones de TensorFlow, solo se usará la CPU\n",
    "# Verificar que realmente no estamos utilizando ninguna GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Comprobar version tensorflow\n",
    "print(tf.__version__)\n",
    "\n",
    "# Comprobar que estamos usando la GPU\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha mencionado anteriormente, el dataset que se va a utilizar es un archivo .npy que contiene las imágenes de los rostros en formato de matriz de píxeles. Para cargar el dataset, se utiliza la función `np.load()` de la librería NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro dataset esta dividido en dos partes, una con las imágenes y otra con las etiquetas. Para la recolección de imágenes se ha utilizado un dataset cuyo escenario experimental consiste en pasos que causan o no causan estrés; lectura de escritura en el idioma nativo, entrevista en el idioma nativo, lectura de escritura en idioma no nativo, entrevista en idioma no nativo.\n",
    "\n",
    "Por lo tanto las distintas carpetas que forma el dataset son:\n",
    "\n",
    "- **Native_Language_Script_Reading** (BAJO ESTRES-NEUTRAL)\n",
    "- **Native_Language_Interview** (BAJO ESTRES)\n",
    "- **Non_Native_Language_Script_Reading** (ALTO ESTRES-NEUTRAL)\n",
    "- **Non_Native_Language_Interview** (ALTO ESTRES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar archivos .npy de la manera que se ha descrito y asignar las etiquetas correspondientes (como \"BAJO ESTRES-NEUTRAL\", \"BAJO ESTRES\", \"ALTO ESTRES-NEUTRAL\", \"ALTO ESTRES\") basadas en el nombre de la carpeta en la que se encuentran, se van a seguir los siguientes pasos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Asignar las etiquetas a las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asignar las etiquetas a las imágenes, se va a utilizar una primero una función que recorra las carpetas del dataset y asigne las etiquetas correspondientes a cada imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignar_etiqueta(nombre_carpeta):\n",
    "    if nombre_carpeta == \"Native_Language_Interview\":\n",
    "        return \"BAJO ESTRES\"\n",
    "    elif nombre_carpeta == \"Native_Language_Script_Reading\":\n",
    "        return \"BAJO ESTRES-NEUTRAL\"\n",
    "    elif nombre_carpeta == \"Non-native_Language_Interview\":\n",
    "        return \"ALTO ESTRES\"\n",
    "    elif nombre_carpeta == \"Non-native_Language_Script_Reading\":\n",
    "        return \"ALTO ESTRES-NEUTRAL\"\n",
    "    else:\n",
    "        return None  # Por si el nombre de la carpeta no coincide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cargar las imágenes y las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataset que se va a utilizar, se van a cargar las imágenes de distintas carpetas, segun la persona que aparece en la imagen, y las etiquetas correspondientes a cada imagen. Dentro de cada carpeta de cada persona, se encuentran las carpetas con las clases de estrés correspondientes, y dentro de cada una de estas carpetas se encuentran las imágenes. Lo que se va a realizar es cargar las imágenes y las etiquetas de cada una de las carpetas de las clases de estrés, y se van a guardar todas las corresponientes a cada persona en un array de numpy. Para asi tener un array de numpy con todas las imágenes y otro con todas las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET/05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET/28\n",
      "DATASET/20\n",
      "DATASET/29\n",
      "DATASET/22\n",
      "DATASET/30\n",
      "DATASET/10\n",
      "DATASET/14\n",
      "DATASET/21\n",
      "DATASET/02\n",
      "DATASET/25\n",
      "DATASET/11\n",
      "DATASET/15\n",
      "DATASET/16\n",
      "DATASET/09\n",
      "DATASET/33\n",
      "DATASET/17\n",
      "DATASET/24\n",
      "DATASET/32\n",
      "DATASET/18\n",
      "DATASET/27\n",
      "DATASET/03\n",
      "DATASET/23\n",
      "DATASET/19\n",
      "DATASET/13\n",
      "DATASET/01\n",
      "DATASET/07\n",
      "DATASET/08\n",
      "DATASET/31\n",
      "DATASET/04\n",
      "DATASET/12\n",
      "DATASET/06\n",
      "Etiqueta 'ALTO ESTRES-NEUTRAL': 16000 imágenes\n",
      "Etiqueta 'BAJO ESTRES-NEUTRAL': 16000 imágenes\n",
      "Etiqueta 'ALTO ESTRES': 16000 imágenes\n",
      "Etiqueta 'BAJO ESTRES': 16000 imágenes\n"
     ]
    }
   ],
   "source": [
    "# Definir la ruta base de tu DATASET\n",
    "ruta = \"DATASET\"\n",
    "\n",
    "# Definir las subcarpetas que corresponden a cada clase\n",
    "classes = [\"Native_Language_Script_Reading\", \"Native_Language_Interview\",\n",
    "           \"Non-native_Language_Script_Reading\", \"Non-native_Language_Interview\"]\n",
    "\n",
    "# Número deseado de imágenes por clase\n",
    "num_images_per_class = 500\n",
    "\n",
    "# Listas para almacenar las imágenes y las etiquetas\n",
    "imagenes = []\n",
    "etiquetas = []\n",
    "\n",
    "# Recorrer cada persona en el dataset\n",
    "for carpeta_persona in os.listdir(ruta):\n",
    "    ruta_persona = os.path.join(ruta, carpeta_persona)\n",
    "    print(ruta_persona)\n",
    "    if os.path.isdir(ruta_persona):  # Asegurar que es un directorio\n",
    "        # Recorrer cada clase\n",
    "        for cls in classes:\n",
    "            ruta_clase = os.path.join(ruta_persona, cls)\n",
    "            if os.path.exists(ruta_clase):\n",
    "                # Lista todas las imágenes (archivos .npy) en la carpeta de la clase\n",
    "                images = [os.path.join(ruta_clase, img) for img in os.listdir(ruta_clase) if img.endswith('.npy')]            \n",
    "                # Seleccionar 1000 imágenes aleatorias sin repetición\n",
    "                if len(images) >= num_images_per_class:\n",
    "                    selected_images = random.sample(images, num_images_per_class)\n",
    "                else:\n",
    "                    selected_images = images  # Si hay menos de 1000, tomar todas\n",
    "                \n",
    "                # Añadir las imágenes seleccionadas y sus etiquetas a las listas\n",
    "                for ruta_imagen in selected_images:\n",
    "                    imagen = np.load(ruta_imagen, allow_pickle=True)\n",
    "                    imagenes.append(imagen)\n",
    "                    etiquetas.append(asignar_etiqueta(cls))\n",
    "                    \n",
    "# Contar etiquetas por clase\n",
    "conteo_etiquetas = {etiqueta: etiquetas.count(etiqueta) for etiqueta in set(etiquetas)}\n",
    "\n",
    "# Número de etiquetas creadas por clase\n",
    "for etiqueta, conteo in conteo_etiquetas.items():\n",
    "    print(f\"Etiqueta '{etiqueta}': {conteo} imágenes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizar las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar con el preprocesamiento de los datos, se van a visualizar algunas de las imágenes del dataset para comprobar que se han cargado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes cargadas: 64000\n",
      "Número de etiquetas cargadas: 64000\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que se han cargado las imágenes\n",
    "print(\"Número de imágenes cargadas:\", len(imagenes))\n",
    "print(\"Número de etiquetas cargadas:\", len(etiquetas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar el número de imágenes que hay en cada categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes por etiqueta: Counter({'BAJO ESTRES-NEUTRAL': 16000, 'BAJO ESTRES': 16000, 'ALTO ESTRES-NEUTRAL': 16000, 'ALTO ESTRES': 16000})\n"
     ]
    }
   ],
   "source": [
    "# Contar cuántas imágenes hay de cada etiqueta\n",
    "from collections import Counter\n",
    "\n",
    "conteo = Counter(etiquetas)\n",
    "print(\"Número de imágenes por etiqueta:\", conteo)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se han cargado las imágenes y las etiquetas, se van a preprocesar los datos para poder utilizarlos en el modelo de clasificación. Para ello, se van a seguir los siguientes pasos:\n",
    "\n",
    "- Normalizar las imágenes\n",
    "- Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "- Codificar las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Normalizar las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 256, 7, 7)\n",
      "(64000,)\n"
     ]
    }
   ],
   "source": [
    "# Convertir la lista de imágenes y etiquetas a un array de Numpy para facilitar su manipulación\n",
    "imagenes = np.array(imagenes, dtype=\"float32\") / 255\n",
    "print(imagenes.shape)\n",
    "\n",
    "etiquetas = np.array(etiquetas)\n",
    "print(etiquetas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Dividir los datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto de entrenamiento, conjunyo de test y conjunto de validación, con una proporción de 3:1:1 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento: (38400, 256, 7, 7) (38400, 4)\n",
      "Datos de validación: (12800, 256, 7, 7) (12800, 4)\n",
      "Datos de prueba: (12800, 256, 7, 7) (12800, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Codificar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "etiquetas_encoded = label_encoder.fit_transform(etiquetas)  # Codificar todas las etiquetas\n",
    "etiquetas_categorical = to_categorical(etiquetas_encoded)  # Convertir a one-hot\n",
    "\n",
    "# Dividir los datos en datos de entrenamiento y temporales (combinando validación y prueba)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(imagenes, etiquetas_categorical, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividir los datos temporales en datos de validación y prueba\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Comprobar el tamaño de los conjuntos de datos\n",
    "print(\"Datos de entrenamiento:\", X_train.shape, y_train.shape)\n",
    "print(\"Datos de validación:\", X_val.shape, y_val.shape)\n",
    "print(\"Datos de prueba:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta manera de dividir los datos nos va a permitir entrenar el modelo con el conjunto de entrenamiento, ajustar los hiperparámetros con el conjunto de validación y evaluar el modelo con el conjunto de prueba.\n",
    "\n",
    "Tambien es crucial para evitar el overfitting, ya que si entrenamos y evaluamos el modelo con el mismo conjunto de datos, el modelo puede aprender a predecir las etiquetas de ese conjunto de datos en lugar de generalizar a nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Codificar las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para modelos de clasificación, especialmente en Keras/TensorFlow, las etiquetas deben estar codificadas de manera que cada etiqueta sea un vector binario. Esto se conoce como codificación one-hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Codificar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "etiquetas_encoded = label_encoder.fit_transform(etiquetas)  # Codificar todas las etiquetas\n",
    "etiquetas_categorical = to_categorical(etiquetas_encoded)  # Convertir a one-hot\n",
    "\n",
    "# Configurar validación cruzada con 5 divisiones\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Variables para almacenar los resultados de la validación cruzada\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se han preprocesado los datos, se va a crear el modelo de clasificación de imágenes de rostros humanos en función del estrés emocional que presentan. Para ello, se va a utilizar una red neuronal densa que consta de varias capas totalmente conectadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definición de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se van a explicar los modelos que se van a utilizar para la clasificación de las imágenes. Todos los modelos que se muestran en este apartado son redes neuronales densas.\n",
    "Todos estos modelos se han elegido debido a que a lo largo del tiempo se han utilizado para diferentes objetivos de clasificación y han obtenido buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Modelo Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Función para crear el modelo\n",
    "def create_model_1():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(256, 7, 7)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  # Asume 4 clases como en tu ejemplo\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilar y entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold número 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:35:24.238369: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 13:35:24.563928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:24.564178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:24.564409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:24.564622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:24.564841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:24.565053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.021956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.022231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.022482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.022712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.022935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.023131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22263 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "2024-07-17 13:35:25.023551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:35:25.023743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6089 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:35:25.398879: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1541406720 exceeds 10% of free system memory.\n",
      "2024-07-17 13:35:26.307511: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1541406720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:35:27.889674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-07-17 13:35:27.892753: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f09e03adc60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-17 13:35:27.892766: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-07-17 13:35:27.892771: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2024-07-17 13:35:27.896229: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-17 13:35:28.010472: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - 4s 3ms/step - loss: 0.3100 - accuracy: 0.8850 - val_loss: 0.1191 - val_accuracy: 0.9639\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.1066 - accuracy: 0.9654 - val_loss: 0.0805 - val_accuracy: 0.9745\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0706 - accuracy: 0.9781 - val_loss: 0.0650 - val_accuracy: 0.9781\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0553 - accuracy: 0.9818 - val_loss: 0.0488 - val_accuracy: 0.9841\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0415 - accuracy: 0.9869 - val_loss: 0.0401 - val_accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0351 - accuracy: 0.9882 - val_loss: 0.0420 - val_accuracy: 0.9852\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9897 - val_loss: 0.0342 - val_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.0330 - val_accuracy: 0.9892\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.0304 - val_accuracy: 0.9904\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.0293 - val_accuracy: 0.9892\n",
      "Score for fold 1: loss of 0.030726486817002296; accuracy of 99.06250238418579%\n",
      "Fold número 2\n",
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:35:53.974873: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1541406720 exceeds 10% of free system memory.\n",
      "2024-07-17 13:35:54.873788: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1541406720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.3064 - accuracy: 0.8884 - val_loss: 0.1235 - val_accuracy: 0.9598\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.1042 - accuracy: 0.9660 - val_loss: 0.0758 - val_accuracy: 0.9755\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0694 - accuracy: 0.9780 - val_loss: 0.0585 - val_accuracy: 0.9794\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0539 - accuracy: 0.9827 - val_loss: 0.0511 - val_accuracy: 0.9826\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.0393 - val_accuracy: 0.9863\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.0371 - val_accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.0337 - val_accuracy: 0.9885\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.9930 - val_loss: 0.0306 - val_accuracy: 0.9895\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0215 - accuracy: 0.9929 - val_loss: 0.0292 - val_accuracy: 0.9897\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0192 - accuracy: 0.9938 - val_loss: 0.0315 - val_accuracy: 0.9882\n",
      "Score for fold 2: loss of 0.033210668712854385; accuracy of 98.95312786102295%\n",
      "Fold número 3\n",
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:36:20.837327: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1541406720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.2962 - accuracy: 0.8943 - val_loss: 0.1215 - val_accuracy: 0.9630\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.1016 - accuracy: 0.9667 - val_loss: 0.0755 - val_accuracy: 0.9772\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0700 - accuracy: 0.9774 - val_loss: 0.0666 - val_accuracy: 0.9770\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0516 - accuracy: 0.9826 - val_loss: 0.0431 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 0.0482 - val_accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0354 - accuracy: 0.9886 - val_loss: 0.0357 - val_accuracy: 0.9887\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0301 - accuracy: 0.9903 - val_loss: 0.0328 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.0306 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0215 - accuracy: 0.9929 - val_loss: 0.0334 - val_accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0173 - accuracy: 0.9950 - val_loss: 0.0332 - val_accuracy: 0.9882\n",
      "Score for fold 3: loss of 0.03544995188713074; accuracy of 98.79687428474426%\n",
      "Fold número 4\n",
      "Training fold 4...\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.3222 - accuracy: 0.8809 - val_loss: 0.1341 - val_accuracy: 0.9578\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.0765 - val_accuracy: 0.9762\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0729 - accuracy: 0.9768 - val_loss: 0.0635 - val_accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0531 - accuracy: 0.9831 - val_loss: 0.0613 - val_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 0.0460 - val_accuracy: 0.9855\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0355 - accuracy: 0.9882 - val_loss: 0.0458 - val_accuracy: 0.9863\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0436 - val_accuracy: 0.9863\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.0572 - val_accuracy: 0.9822\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.0357 - val_accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.0347 - val_accuracy: 0.9893\n",
      "Score for fold 4: loss of 0.02954978495836258; accuracy of 99.09374713897705%\n",
      "Fold número 5\n",
      "Training fold 5...\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.3111 - accuracy: 0.8876 - val_loss: 0.1295 - val_accuracy: 0.9592\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 3s 3ms/step - loss: 0.1080 - accuracy: 0.9658 - val_loss: 0.0741 - val_accuracy: 0.9755\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.0476 - val_accuracy: 0.9852\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9868\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0428 - accuracy: 0.9857 - val_loss: 0.0361 - val_accuracy: 0.9893\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 2s 3ms/step - loss: 0.0339 - accuracy: 0.9888 - val_loss: 0.0366 - val_accuracy: 0.9876\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.0296 - val_accuracy: 0.9909\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.0294 - val_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.0295 - val_accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 2s 2ms/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.0274 - val_accuracy: 0.9914\n",
      "Score for fold 5: loss of 0.03163372352719307; accuracy of 98.96093606948853%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "# Validación cruzada, iterar sobre cada división\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    print(f\"Fold número {fold_no}\")\n",
    "    # Usar las divisiones correctas para el entrenamiento y la validación\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = create_model_1()\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Entrenar el modelo y validar con los datos de validación\n",
    "    print(f'Training fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold))\n",
    "    \n",
    "    # Evaluar el modelo con los datos de test\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    #Incrementar el número de la división\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.030726486817002296 - Accuracy: 99.06250238418579%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.033210668712854385 - Accuracy: 98.95312786102295%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.03544995188713074 - Accuracy: 98.79687428474426%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.02954978495836258 - Accuracy: 99.09374713897705%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.03163372352719307 - Accuracy: 98.96093606948853%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 98.97343754768372 (+- 0.10406757324686733)\n",
      "> Loss: 0.032114123180508616\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la validación cruzada\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "# Reiniciar las variables para almacenar los resultados de la validación cruzada de otro modelo\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Modelo LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Asumiendo que tus datos son características extraídas (256 características por cada una de las 7x7 \"imágenes\")\n",
    "def create_model_2():\n",
    "    model_LeNet = Sequential([\n",
    "        Flatten(input_shape=(256, 7, 7)), # Aplanamos las características para hacerlas compatibles con capas densas\n",
    "        Dense(512, activation='relu'), # Aumentamos la dimensión. Ajusta según necesidad.\n",
    "        Dropout(0.5), # Ayuda a prevenir el sobreajuste\n",
    "        Dense(120, activation='relu'), # Capa densa con 120 nodos como en LeNet\n",
    "        Dense(84, activation='relu'), # Capa densa con 84 nodos como en LeNet\n",
    "        Dense(4, activation='softmax') # Capa de salida para 4 clases\n",
    "    ])\n",
    "    return model_LeNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilar y entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold número 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:30:05.981514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 13:30:06.294483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.294776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.294995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.295204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.295412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.295624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.857245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.857512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.865911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.866166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.866411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.866780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22263 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "2024-07-17 13:30:06.867482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-17 13:30:06.867672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6089 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2024-07-17 13:30:07.235653: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2312110080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:30:08.578458: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2312110080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:30:10.750967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-07-17 13:30:10.763362: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f798e7d2da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-17 13:30:10.763376: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-07-17 13:30:10.763380: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2024-07-17 13:30:10.774610: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-17 13:30:10.958022: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440/1440 [==============================] - 6s 3ms/step - loss: 0.2273 - accuracy: 0.9050 - val_loss: 0.3440 - val_accuracy: 0.8549\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0949 - accuracy: 0.9654 - val_loss: 0.4328 - val_accuracy: 0.8383\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 3s 2ms/step - loss: 0.0860 - accuracy: 0.9685 - val_loss: 0.8030 - val_accuracy: 0.8400\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 3s 2ms/step - loss: 0.0760 - accuracy: 0.9725 - val_loss: 0.3864 - val_accuracy: 0.8514\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0662 - accuracy: 0.9762 - val_loss: 0.4257 - val_accuracy: 0.8570\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0632 - accuracy: 0.9771 - val_loss: 0.5912 - val_accuracy: 0.8318\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 4s 2ms/step - loss: 0.0599 - accuracy: 0.9789 - val_loss: 0.6588 - val_accuracy: 0.8090\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0603 - accuracy: 0.9781 - val_loss: 0.4297 - val_accuracy: 0.8457\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0549 - accuracy: 0.9806 - val_loss: 0.4242 - val_accuracy: 0.8566\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.7493 - val_accuracy: 0.8232\n",
      "Score for fold 1: loss of 0.10646235197782516; accuracy of 97.24218845367432%\n",
      "Fold número 2\n",
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:30:50.573307: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2312110080 exceeds 10% of free system memory.\n",
      "2024-07-17 13:30:51.906764: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2312110080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1440/1440 [==============================] - 5s 3ms/step - loss: 0.2218 - accuracy: 0.9099 - val_loss: 0.4359 - val_accuracy: 0.8541\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0958 - accuracy: 0.9658 - val_loss: 0.3700 - val_accuracy: 0.8605\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0795 - accuracy: 0.9717 - val_loss: 0.6383 - val_accuracy: 0.8463\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.6876 - val_accuracy: 0.8422\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0699 - accuracy: 0.9749 - val_loss: 0.3395 - val_accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0581 - accuracy: 0.9791 - val_loss: 0.6770 - val_accuracy: 0.8354\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 4s 2ms/step - loss: 0.0589 - accuracy: 0.9787 - val_loss: 0.4475 - val_accuracy: 0.8443\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0537 - accuracy: 0.9809 - val_loss: 0.4648 - val_accuracy: 0.8408\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0550 - accuracy: 0.9794 - val_loss: 0.7873 - val_accuracy: 0.8420\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0523 - accuracy: 0.9806 - val_loss: 0.5884 - val_accuracy: 0.8520\n",
      "Score for fold 2: loss of 0.08546216040849686; accuracy of 97.67968654632568%\n",
      "Fold número 3\n",
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:31:34.116992: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2312110080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1440/1440 [==============================] - 5s 3ms/step - loss: 0.2239 - accuracy: 0.9076 - val_loss: 0.4892 - val_accuracy: 0.8398\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0998 - accuracy: 0.9643 - val_loss: 0.4544 - val_accuracy: 0.8328\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0797 - accuracy: 0.9715 - val_loss: 0.4252 - val_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0692 - accuracy: 0.9757 - val_loss: 0.5209 - val_accuracy: 0.8389\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.5939 - val_accuracy: 0.8355\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0640 - accuracy: 0.9766 - val_loss: 0.8230 - val_accuracy: 0.8232\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0568 - accuracy: 0.9787 - val_loss: 0.4804 - val_accuracy: 0.8439\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0627 - accuracy: 0.9780 - val_loss: 0.4148 - val_accuracy: 0.8584\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0529 - accuracy: 0.9816 - val_loss: 0.4195 - val_accuracy: 0.8428\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0433 - accuracy: 0.9849 - val_loss: 0.5891 - val_accuracy: 0.8328\n",
      "Score for fold 3: loss of 0.09387540817260742; accuracy of 97.48437404632568%\n",
      "Fold número 4\n",
      "Training fold 4...\n",
      "Epoch 1/10\n",
      "1440/1440 [==============================] - 5s 3ms/step - loss: 0.2222 - accuracy: 0.9096 - val_loss: 0.2803 - val_accuracy: 0.8637\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0964 - accuracy: 0.9649 - val_loss: 0.5172 - val_accuracy: 0.8438\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0839 - accuracy: 0.9699 - val_loss: 0.4808 - val_accuracy: 0.8178\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0717 - accuracy: 0.9746 - val_loss: 0.4895 - val_accuracy: 0.8410\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 3s 2ms/step - loss: 0.0738 - accuracy: 0.9731 - val_loss: 0.4732 - val_accuracy: 0.8449\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0720 - accuracy: 0.9737 - val_loss: 0.7127 - val_accuracy: 0.8412\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0608 - accuracy: 0.9784 - val_loss: 0.5844 - val_accuracy: 0.8416\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.3437 - val_accuracy: 0.8662\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0561 - accuracy: 0.9788 - val_loss: 0.7723 - val_accuracy: 0.8377\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0489 - accuracy: 0.9826 - val_loss: 0.4703 - val_accuracy: 0.8424\n",
      "Score for fold 4: loss of 0.07565674185752869; accuracy of 97.60937690734863%\n",
      "Fold número 5\n",
      "Training fold 5...\n",
      "Epoch 1/10\n",
      "1440/1440 [==============================] - 5s 3ms/step - loss: 0.2241 - accuracy: 0.9063 - val_loss: 0.3553 - val_accuracy: 0.8555\n",
      "Epoch 2/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0948 - accuracy: 0.9656 - val_loss: 0.3897 - val_accuracy: 0.8473\n",
      "Epoch 3/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0841 - accuracy: 0.9698 - val_loss: 0.4305 - val_accuracy: 0.8395\n",
      "Epoch 4/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0706 - accuracy: 0.9751 - val_loss: 0.5396 - val_accuracy: 0.8393\n",
      "Epoch 5/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0602 - accuracy: 0.9794 - val_loss: 0.5213 - val_accuracy: 0.8285\n",
      "Epoch 6/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0671 - accuracy: 0.9750 - val_loss: 0.5179 - val_accuracy: 0.8443\n",
      "Epoch 7/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0560 - accuracy: 0.9795 - val_loss: 0.3695 - val_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0611 - accuracy: 0.9772 - val_loss: 0.5514 - val_accuracy: 0.8475\n",
      "Epoch 9/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0507 - accuracy: 0.9809 - val_loss: 0.3841 - val_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "1440/1440 [==============================] - 4s 3ms/step - loss: 0.0540 - accuracy: 0.9795 - val_loss: 0.5919 - val_accuracy: 0.8252\n",
      "Score for fold 5: loss of 0.0886237695813179; accuracy of 97.19531536102295%\n"
     ]
    }
   ],
   "source": [
    "# Antes de entrenar el modelo, debes compilarlo, especificando la función de pérdida y el optimizador que utilizarás.\n",
    "# Aplicar la codificación one-hot\n",
    "for train_index, test_index in kf.split(imagenes):\n",
    "    print(f\"Fold número {fold_no}\")\n",
    "    X_train, X_test = imagenes[train_index], imagenes[test_index]\n",
    "    y_train, y_test = etiquetas_categorical[train_index], etiquetas_categorical[test_index]\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = create_model_2()\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    print(f'Training fold {fold_no}...')\n",
    "    history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "    \n",
    "    # Evaluar el modelo con los datos de prueba\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    #Incrementar el número de la división\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10646235197782516 - Accuracy: 97.24218845367432%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.08546216040849686 - Accuracy: 97.67968654632568%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.09387540817260742 - Accuracy: 97.48437404632568%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.07565674185752869 - Accuracy: 97.60937690734863%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.0886237695813179 - Accuracy: 97.19531536102295%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 97.44218826293945 (+- 0.1934338497561697)\n",
      "> Loss: 0.0900160863995552\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la validación cruzada\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
